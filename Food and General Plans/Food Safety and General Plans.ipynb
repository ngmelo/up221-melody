{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b67867c-3677-4117-b953-662afb9d5b76",
   "metadata": {},
   "source": [
    "# Food Safety and Community Food Production\n",
    "\n",
    "An exploration of how narratives of \"safety\" in land use policy impact urban agriculture in select CA Counties: LA, Ventura, Sonoma, Mendocino, and Lake Counties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd96ed-3151-4999-834d-f5423eac22d6",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "112 total general plans from the five CA Counties (LA, Ventura, Sonoma, Mendocino, Lake)\n",
    "\n",
    "\n",
    "1. Basic word counts:\n",
    "\n",
    "2. Modified/precision topic modeling: How often do mentions of food, agriculture, soil health, food safety feature in plans? And as important, where are they mentioned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7b07ea-d342-4842-999a-e986ee44756e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "# visualizations\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# census\n",
    "import cenpy\n",
    "from cenpy import products\n",
    "\n",
    "# set display\n",
    "pd.options.display.max_columns = 150\n",
    "#pd.options.display.max_rows = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2756d0fd-304b-43d0-a40e-ed9c20894ec0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LdaMulticore in module gensim.models.ldamulticore:\n",
      "\n",
      "class LdaMulticore(gensim.models.ldamodel.LdaModel)\n",
      " |  LdaMulticore(corpus=None, num_topics=100, id2word=None, workers=None, chunksize=2000, passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, random_state=None, minimum_probability=0.01, minimum_phi_value=0.01, per_word_topics=False, dtype=<class 'numpy.float32'>)\n",
      " |  \n",
      " |  An optimized implementation of the LDA algorithm, able to harness the power of multicore CPUs.\n",
      " |  Follows the similar API as the parent class :class:`~gensim.models.ldamodel.LdaModel`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaMulticore\n",
      " |      gensim.models.ldamodel.LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      gensim.models.basemodel.BaseTopicModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, workers=None, chunksize=2000, passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, random_state=None, minimum_probability=0.01, minimum_phi_value=0.01, per_word_topics=False, dtype=<class 'numpy.float32'>)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\n",
      " |          If not given, the model is left untrained (presumably because you want to call\n",
      " |          :meth:`~gensim.models.ldamodel.LdaModel.update` manually).\n",
      " |      num_topics : int, optional\n",
      " |          The number of requested latent topics to be extracted from the training corpus.\n",
      " |      id2word : {dict of (int, str),  :class:`gensim.corpora.dictionary.Dictionary`}\n",
      " |          Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\n",
      " |          debugging and topic printing.\n",
      " |      workers : int, optional\n",
      " |          Number of workers processes to be used for parallelization. If None all available cores\n",
      " |          (as estimated by `workers=cpu_count()-1` will be used. **Note** however that for\n",
      " |          hyper-threaded CPUs, this estimation returns a too high number -- set `workers`\n",
      " |          directly to the number of your **real** cores (not hyperthreads) minus one, for optimal performance.\n",
      " |      chunksize :  int, optional\n",
      " |          Number of documents to be used in each training chunk.\n",
      " |      passes : int, optional\n",
      " |          Number of passes through the corpus during training.\n",
      " |      alpha : {float, numpy.ndarray of float, list of float, str}, optional\n",
      " |          A-priori belief on document-topic distribution, this can be:\n",
      " |              * scalar for a symmetric prior over document-topic distribution,\n",
      " |              * 1D array of length equal to num_topics to denote an asymmetric user defined prior for each topic.\n",
      " |      \n",
      " |          Alternatively default prior selecting strategies can be employed by supplying a string:\n",
      " |              * 'symmetric': (default) Uses a fixed symmetric prior of `1.0 / num_topics`,\n",
      " |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / (topic_index + sqrt(num_topics))`.\n",
      " |      eta : {float, numpy.ndarray of float, list of float, str}, optional\n",
      " |          A-priori belief on topic-word distribution, this can be:\n",
      " |              * scalar for a symmetric prior over topic-word distribution,\n",
      " |              * 1D array of length equal to num_words to denote an asymmetric user defined prior for each word,\n",
      " |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination.\n",
      " |      \n",
      " |          Alternatively default prior selecting strategies can be employed by supplying a string:\n",
      " |              * 'symmetric': (default) Uses a fixed symmetric prior of `1.0 / num_topics`,\n",
      " |              * 'auto': Learns an asymmetric prior from the corpus.\n",
      " |      decay : float, optional\n",
      " |          A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n",
      " |          when each new document is examined. Corresponds to :math:`\\kappa` from\n",
      " |          `'Online Learning for LDA' by Hoffman et al.`_\n",
      " |      offset : float, optional\n",
      " |          Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n",
      " |          Corresponds to :math:`\\tau_0` from `'Online Learning for LDA' by Hoffman et al.`_\n",
      " |      eval_every : int, optional\n",
      " |          Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
      " |      iterations : int, optional\n",
      " |          Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
      " |      gamma_threshold : float, optional\n",
      " |          Minimum change in the value of the gamma parameters to continue iterating.\n",
      " |      minimum_probability : float, optional\n",
      " |          Topics with a probability lower than this threshold will be filtered out.\n",
      " |      random_state : {np.random.RandomState, int}, optional\n",
      " |          Either a randomState object or a seed to generate one. Useful for reproducibility.\n",
      " |          Note that results can still vary due to non-determinism in OS scheduling of the worker processes.\n",
      " |      minimum_phi_value : float, optional\n",
      " |          if `per_word_topics` is True, this represents a lower bound on the term probabilities.\n",
      " |      per_word_topics : bool\n",
      " |          If True, the model also computes a list of topics, sorted in descending order of most likely topics for\n",
      " |          each word, along with their phi values multiplied by the feature length (i.e. word count).\n",
      " |      dtype : {numpy.float16, numpy.float32, numpy.float64}, optional\n",
      " |          Data-type to use during calculations inside model. All inputs are also converted.\n",
      " |  \n",
      " |  update(self, corpus, chunks_as_numpy=False)\n",
      " |      Train the model with new documents, by EM-iterating over `corpus` until the topics converge\n",
      " |      (or until the maximum number of allowed iterations is reached).\n",
      " |      \n",
      " |      Train the model with new documents, by EM-iterating over the corpus until the topics converge, or until\n",
      " |      the maximum number of allowed iterations is reached. `corpus` must be an iterable. The E step is distributed\n",
      " |      into the several processes.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This update also supports updating an already trained model (`self`) with new documents from `corpus`;\n",
      " |      the two models are then merged in proportion to the number of old vs. new documents.\n",
      " |      This feature is still experimental for non-stationary input streams.\n",
      " |      \n",
      " |      For stationary input (no topic drift in new documents), on the other hand,\n",
      " |      this equals the online update of `'Online Learning for LDA' by Hoffman et al.`_\n",
      " |      and is guaranteed to converge for any `decay` in (0.5, 1].\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`) used to update the\n",
      " |          model.\n",
      " |      chunks_as_numpy : bool\n",
      " |          Whether each chunk passed to the inference step should be a np.ndarray or not. Numpy can in some settings\n",
      " |          turn the term IDs into floats, these will be converted back into integers in inference, which incurs a\n",
      " |          performance hit. For distributed computing it may be desirable to keep the chunks as `numpy.ndarray`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.ldamodel.LdaModel:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=None)\n",
      " |      Get the topic distribution for the given document.\n",
      " |      \n",
      " |      Wraps :meth:`~gensim.models.ldamodel.LdaModel.get_document_topics` to support an operator style call.\n",
      " |      Uses the model's current state (set using constructor arguments) to fill in the additional arguments of the\n",
      " |      wrapper method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ---------\n",
      " |      bow : list of (int, float)\n",
      " |          The document in BOW format.\n",
      " |      eps : float, optional\n",
      " |          Topics with an assigned probability lower than this threshold will be discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Topic distribution for the given document. Each topic is represented as a pair of its ID and the probability\n",
      " |          assigned to it.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Get a string representation of the current object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the most important model parameters.\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from the corpus as E_q[log p(corpus)] - E_q[log q(corpus)].\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of list of (int, float), optional\n",
      " |          Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`) used to estimate the\n",
      " |          variational bounds.\n",
      " |      gamma : numpy.ndarray, optional\n",
      " |          Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\n",
      " |      subsample_ratio : float, optional\n",
      " |          Percentage of the whole corpus represented by the passed `corpus` argument (in case this was a sample).\n",
      " |          Set to 1.0 if the whole corpus was passed.This is used as a multiplicative factor to scale the likelihood\n",
      " |          appropriately.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The variational bound score calculated for each document.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear the model's state to free some memory. Used in the distributed implementation.\n",
      " |  \n",
      " |  diff(self, other, distance='kullback_leibler', num_words=100, n_ann_terms=10, diagonal=False, annotation=True, normed=True)\n",
      " |      Calculate the difference in topic distributions between two models: `self` and `other`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`~gensim.models.ldamodel.LdaModel`\n",
      " |          The model which will be compared against the current object.\n",
      " |      distance : {'kullback_leibler', 'hellinger', 'jaccard', 'jensen_shannon'}\n",
      " |          The distance metric to calculate the difference with.\n",
      " |      num_words : int, optional\n",
      " |          The number of most relevant words used if `distance == 'jaccard'`. Also used for annotating topics.\n",
      " |      n_ann_terms : int, optional\n",
      " |          Max number of words in intersection/symmetric difference between topics. Used for annotation.\n",
      " |      diagonal : bool, optional\n",
      " |          Whether we need the difference between identical topics (the diagonal of the difference matrix).\n",
      " |      annotation : bool, optional\n",
      " |          Whether the intersection or difference of words between two topics should be returned.\n",
      " |      normed : bool, optional\n",
      " |          Whether the matrix should be normalized or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          A difference matrix. Each element corresponds to the difference between the two topics,\n",
      " |          shape (`self.num_topics`, `other.num_topics`)\n",
      " |      numpy.ndarray, optional\n",
      " |          Annotation matrix where for each pair we include the word from the intersection of the two topics,\n",
      " |          and the word from the symmetric difference of the two topics. Only included if `annotation == True`.\n",
      " |          Shape (`self.num_topics`, `other_model.num_topics`, 2).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Get the differences between each pair of topics inferred by two models\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models.ldamulticore import LdaMulticore\n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> m1 = LdaMulticore.load(datapath(\"lda_3_0_1_model\"))\n",
      " |          >>> m2 = LdaMulticore.load(datapath(\"ldamodel_python_3_5\"))\n",
      " |          >>> mdiff, annotation = m1.diff(m2)\n",
      " |          >>> topic_diff = mdiff  # get matrix with difference for each topic pair from `m1` and `m2`\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected sufficient statistics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : list of list of (int, float)\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      state : :class:`~gensim.models.ldamodel.LdaState`, optional\n",
      " |          The state to be updated with the newly accumulated sufficient statistics. If none, the models\n",
      " |          `self.state` is updated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Gamma parameters controlling the topic weights, shape (`len(chunk)`, `self.num_topics`).\n",
      " |  \n",
      " |  do_mstep(self, rho, other, extra_pass=False)\n",
      " |      Maximization step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      other : :class:`~gensim.models.ldamodel.LdaModel`\n",
      " |          The model whose sufficient statistics will be used to update the topics.\n",
      " |      extra_pass : bool, optional\n",
      " |          Whether this step required an additional pass over the corpus.\n",
      " |  \n",
      " |  get_document_topics(self, bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
      " |      Get the topic distribution for the given document.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bow : corpus : list of (int, float)\n",
      " |          The document in BOW format.\n",
      " |      minimum_probability : float\n",
      " |          Topics with an assigned probability lower than this threshold will be discarded.\n",
      " |      minimum_phi_value : float\n",
      " |          If `per_word_topics` is True, this represents a lower bound on the term probabilities that are included.\n",
      " |           If set to None, a value of 1e-8 is used to prevent 0s.\n",
      " |      per_word_topics : bool\n",
      " |          If True, this function will also return two extra lists as explained in the \"Returns\" section.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Topic distribution for the whole document. Each element in the list is a pair of a topic's id, and\n",
      " |          the probability that was assigned to it.\n",
      " |      list of (int, list of (int, float), optional\n",
      " |          Most probable topics per word. Each element in the list is a pair of a word's id, and a list of\n",
      " |          topics sorted by their relevance to this word. Only returned if `per_word_topics` was set to True.\n",
      " |      list of (int, list of float), optional\n",
      " |          Phi relevance values, multiplied by the feature length, for each word-topic combination.\n",
      " |          Each element in the list is a pair of a word's id and a list of the phi values between this word and\n",
      " |          each topic. Only returned if `per_word_topics` was set to True.\n",
      " |  \n",
      " |  get_term_topics(self, word_id, minimum_probability=None)\n",
      " |      Get the most relevant topics to the given word.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_id : int\n",
      " |          The word for which the topic distribution will be computed.\n",
      " |      minimum_probability : float, optional\n",
      " |          Topics with an assigned probability below this threshold will be discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          The relevant topics represented as pairs of their ID and their assigned probability, sorted\n",
      " |          by relevance to the given word.\n",
      " |  \n",
      " |  get_topic_terms(self, topicid, topn=10)\n",
      " |      Get the representation for a single topic. Words the integer IDs, in constrast to\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          The ID of the topic to be returned\n",
      " |      topn : int, optional\n",
      " |          Number of the most significant words that are associated with the topic.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, float)\n",
      " |          Word ID - probability pairs for the most relevant words generated by the topic.\n",
      " |  \n",
      " |  get_topics(self)\n",
      " |      Get the term-topic matrix learned during inference.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The probability for each word in each topic, shape (`num_topics`, `vocabulary_size`).\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters controlling the topic weights)\n",
      " |      for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model. The whole input chunk of document is assumed to fit in RAM;\n",
      " |      chunking of a large corpus must be done earlier in the pipeline. Avoids computing the `phi` variational\n",
      " |      parameter directly using the optimization presented in\n",
      " |      `Lee, Seung: Algorithms for non-negative matrix factorization\"\n",
      " |      <https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : list of list of (int, float)\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      collect_sstats : bool, optional\n",
      " |          If set to True, also collect (and return) sufficient statistics needed to update the model's topic-word\n",
      " |          distributions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      (numpy.ndarray, {numpy.ndarray, None})\n",
      " |          The first element is always returned and it corresponds to the states gamma matrix. The second element is\n",
      " |          only returned if `collect_sstats` == True and corresponds to the sufficient statistics for the M step.\n",
      " |  \n",
      " |  init_dir_prior(self, prior, name)\n",
      " |      Initialize priors for the Dirichlet distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prior : {float, numpy.ndarray of float, list of float, str}\n",
      " |          A-priori belief on document-topic distribution. If `name` == 'alpha', then the prior can be:\n",
      " |              * scalar for a symmetric prior over document-topic distribution,\n",
      " |              * 1D array of length equal to num_topics to denote an asymmetric user defined prior for each topic.\n",
      " |      \n",
      " |          Alternatively default prior selecting strategies can be employed by supplying a string:\n",
      " |              * 'symmetric': (default) Uses a fixed symmetric prior of `1.0 / num_topics`,\n",
      " |              * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / (topic_index + sqrt(num_topics))`,\n",
      " |              * 'auto': Learns an asymmetric prior from the corpus (not available if `distributed==True`).\n",
      " |      \n",
      " |          A-priori belief on topic-word distribution. If `name` == 'eta' then the prior can be:\n",
      " |              * scalar for a symmetric prior over topic-word distribution,\n",
      " |              * 1D array of length equal to num_words to denote an asymmetric user defined prior for each word,\n",
      " |              * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination.\n",
      " |      \n",
      " |          Alternatively default prior selecting strategies can be employed by supplying a string:\n",
      " |              * 'symmetric': (default) Uses a fixed symmetric prior of `1.0 / num_topics`,\n",
      " |              * 'auto': Learns an asymmetric prior from the corpus.\n",
      " |      name : {'alpha', 'eta'}\n",
      " |          Whether the `prior` is parameterized by the alpha vector (1 parameter per topic)\n",
      " |          or by the eta (1 parameter per unique term in the vocabulary).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      init_prior: numpy.ndarray\n",
      " |          Initialized Dirichlet prior:\n",
      " |          If 'alpha' was provided as `name` the shape is (self.num_topics, ).\n",
      " |          If 'eta' was provided as `name` the shape is (len(self.id2word), ).\n",
      " |      is_auto: bool\n",
      " |          Flag that shows if hyperparameter optimization should be used or not.\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using a chunk of documents as evaluation corpus.\n",
      " |      \n",
      " |      Also output the calculated statistics, including the perplexity=2^(-bound), to log at INFO level.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : list of list of (int, float)\n",
      " |          The corpus chunk on which the inference step will be performed.\n",
      " |      total_docs : int, optional\n",
      " |          Number of docs used for evaluation of the perplexity.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The variational bound score calculated for each word.\n",
      " |  \n",
      " |  save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you intend to use models across Python 2/3 versions there are a few things to\n",
      " |      keep in mind:\n",
      " |      \n",
      " |        1. The pickled Python dictionaries will not work across Python versions\n",
      " |        2. The `save` method does not automatically save all numpy arrays separately, only\n",
      " |           those ones that exceed `sep_limit` set in :meth:`~gensim.utils.SaveLoad.save`. The main\n",
      " |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      " |      \n",
      " |      Please refer to the `wiki recipes section\n",
      " |      <https://github.com/RaRe-Technologies/gensim/wiki/\n",
      " |      Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2>`_\n",
      " |      for an example on how to work around these issues.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.load`\n",
      " |          Load model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the system file where the model will be persisted.\n",
      " |      ignore : tuple of str, optional\n",
      " |          The named attributes in the tuple will be left out of the pickled model. The reason why\n",
      " |          the internal `state` is ignored by default is that it uses its own serialisation rather than the one\n",
      " |          provided by this method.\n",
      " |      separately : {list of str, None}, optional\n",
      " |          If None -  automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This avoids pickle memory errors and allows `mmap`'ing large arrays\n",
      " |          back on load efficiently. If list of str - this attributes will be stored in separate files,\n",
      " |          the automatic check is not performed in this case.\n",
      " |      *args\n",
      " |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |      **kwargs\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Get the representation for a single topic. Words here are the actual strings, in constrast to\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.get_topic_terms` that represents words by their vocabulary ID.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicid : int\n",
      " |          The ID of the topic to be returned\n",
      " |      topn : int, optional\n",
      " |          Number of the most significant words that are associated with the topic.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          Word - probability pairs for the most relevant words generated by the topic.\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      Get a representation for selected topics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          Number of topics to be returned. Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |          The returned topics subset of all topics is therefore arbitrary and may change between two LDA\n",
      " |          training runs.\n",
      " |      num_words : int, optional\n",
      " |          Number of words to be presented for each topic. These will be the most relevant words (assigned the highest\n",
      " |          probability for each topic).\n",
      " |      log : bool, optional\n",
      " |          Whether the output is also logged, besides being returned.\n",
      " |      formatted : bool, optional\n",
      " |          Whether the topic representations should be formatted as strings. If False, they are returned as\n",
      " |          2 tuples of (word, probability).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of {str, tuple of (str, float)}\n",
      " |          a list of topics, each represented either as a string (when `formatted` == True) or word-probability\n",
      " |          pairs.\n",
      " |  \n",
      " |  sync_state(self, current_Elogbeta=None)\n",
      " |      Propagate the states topic probabilities to the inner object's attribute.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      current_Elogbeta: numpy.ndarray\n",
      " |          Posterior probabilities for each topic, optional.\n",
      " |          If omitted, it will get Elogbeta from state.\n",
      " |  \n",
      " |  top_topics(self, corpus=None, texts=None, dictionary=None, window_size=None, coherence='u_mass', topn=20, processes=-1)\n",
      " |      Get the topics with the highest coherence score the coherence for each topic.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of list of (int, float), optional\n",
      " |          Corpus in BoW format.\n",
      " |      texts : list of list of str, optional\n",
      " |          Tokenized texts, needed for coherence models that use sliding window based (i.e. coherence=`c_something`)\n",
      " |          probability estimator .\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
      " |          Gensim dictionary mapping of id word to create corpus.\n",
      " |          If `model.id2word` is present, this is not needed. If both are provided, passed `dictionary` will be used.\n",
      " |      window_size : int, optional\n",
      " |          Is the size of the window to be used for coherence measures using boolean sliding window as their\n",
      " |          probability estimator. For 'u_mass' this doesn't matter.\n",
      " |          If None - the default window sizes are used which are: 'c_v' - 110, 'c_uci' - 10, 'c_npmi' - 10.\n",
      " |      coherence : {'u_mass', 'c_v', 'c_uci', 'c_npmi'}, optional\n",
      " |          Coherence measure to be used.\n",
      " |          Fastest method - 'u_mass', 'c_uci' also known as `c_pmi`.\n",
      " |          For 'u_mass' corpus should be provided, if texts is provided, it will be converted to corpus\n",
      " |          using the dictionary. For 'c_v', 'c_uci' and 'c_npmi' `texts` should be provided (`corpus` isn't needed)\n",
      " |      topn : int, optional\n",
      " |          Integer corresponding to the number of top words to be extracted from each topic.\n",
      " |      processes : int, optional\n",
      " |          Number of processes to use for probability estimation phase, any value less than 1 will be interpreted as\n",
      " |          num_cpus - 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (list of (int, str), float)\n",
      " |          Each element in the list is a pair of a topic representation and its coherence score. Topic representations\n",
      " |          are distributions of words, represented as a list of pairs of word IDs and their probabilities.\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document topic weights.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      gammat : numpy.ndarray\n",
      " |          Previous topic weight parameters.\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Sequence of alpha parameters.\n",
      " |  \n",
      " |  update_eta(self, lambdat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-topic word weights.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lambdat : numpy.ndarray\n",
      " |          Previous lambda parameters.\n",
      " |      rho : float\n",
      " |          Learning rate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          The updated eta parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.ldamodel.LdaModel:\n",
      " |  \n",
      " |  load(fname, *args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`gensim.models.ldamodel.LdaModel` from file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.ldamodel.LdaModel.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file where the model is stored.\n",
      " |      *args\n",
      " |          Positional arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n",
      " |      **kwargs\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.load`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> fname = datapath(\"lda_3_0_1_model\")\n",
      " |          >>> lda = LdaModel.load(fname, mmap='r')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n",
      " |  \n",
      " |  print_topic(self, topicno, topn=10)\n",
      " |      Get a single topic as a formatted string.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicno : int\n",
      " |          Topic id.\n",
      " |      topn : int\n",
      " |          Number of words from topic that will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          String representation of topic, like '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + ... '.\n",
      " |  \n",
      " |  print_topics(self, num_topics=20, num_words=10)\n",
      " |      Get the most significant topics (alias for `show_topics()` method).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          The number of topics to be selected, if -1 - all topics will be in result (ordered by significance).\n",
      " |      num_words : int, optional\n",
      " |          The number of words to be included per topics (ordered by significance).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, list of (str, float))\n",
      " |          Sequence with (topic_id, [(word, value), ... ]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import os\n",
    "\n",
    "# extract text\n",
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "\n",
    "# stop words; split sentences; stems\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# topic modeling\n",
    "import gensim\n",
    "help(gensim.models.LdaMulticore)\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf787bb7-a3c4-4eb5-abf1-51bc527a13dc",
   "metadata": {},
   "source": [
    "#### The Plan Elements\n",
    "\n",
    "After downloading all plans, the plans were split by element and only the \"Land Use\" and \"Open Space/Conservation\" Elements will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19504bb2-913b-4722-8837-9e0f9c8fc13e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Agoura Hills.pdf',\n",
       " 'Alhambra.pdf',\n",
       " 'Arcadia.pdf',\n",
       " 'Artesia.pdf',\n",
       " 'Avalon.pdf',\n",
       " 'Azusa.pdf',\n",
       " 'Baldwin Park.pdf',\n",
       " 'Bell Gardens.pdf',\n",
       " 'Bell.pdf',\n",
       " 'Bellflower.pdf',\n",
       " 'Beverly Hills.pdf',\n",
       " 'Bradbury.pdf',\n",
       " 'Burbank.pdf',\n",
       " 'Calabasas.pdf',\n",
       " 'Camarillo.pdf',\n",
       " 'Carson.pdf',\n",
       " 'Cerritos.pdf',\n",
       " 'Claremont.pdf',\n",
       " 'Clearlake.pdf',\n",
       " 'Commerce.PDF',\n",
       " 'Compton.pdf',\n",
       " 'Cotati.pdf',\n",
       " 'Covina.pdf',\n",
       " 'Cudahy.pdf',\n",
       " 'Culver City.pdf',\n",
       " 'Diamond Bar.pdf',\n",
       " 'Downey.pdf',\n",
       " 'Duarte.pdf',\n",
       " 'El Monte.pdf',\n",
       " 'El Segundo.pdf',\n",
       " 'Filmore.pdf',\n",
       " 'Fort Bragg.pdf',\n",
       " 'Glendale.pdf',\n",
       " 'Hawaiian Gardens.pdf',\n",
       " 'Hawthorne.pdf',\n",
       " 'Healdsburg.pdf',\n",
       " 'Hermosa Beach.pdf',\n",
       " 'Hidden Hills.pdf',\n",
       " 'Huntington Park.pdf',\n",
       " 'Industry.pdf',\n",
       " 'Inglewood.pdf',\n",
       " 'Irwindale.pdf',\n",
       " 'La Canada Flintridge.pdf',\n",
       " 'La Habra Heights.pdf',\n",
       " 'La Mirada.pdf',\n",
       " 'La Puente.pdf',\n",
       " 'La Verne.pdf',\n",
       " 'Lakeport.pdf',\n",
       " 'Lancaster.pdf',\n",
       " 'Lawndale.pdf',\n",
       " 'Lomita.pdf',\n",
       " 'Long Beach.pdf',\n",
       " 'Lynwood.pdf',\n",
       " 'Malibu.pdf',\n",
       " 'Manhattan Beach.pdf',\n",
       " 'Maywood.pdf',\n",
       " 'Monrovia.pdf',\n",
       " 'Montebello.pdf',\n",
       " 'Monterey Park.pdf',\n",
       " 'Moorpark.pdf',\n",
       " 'Norwalk.pdf',\n",
       " 'Ojai.pdf',\n",
       " 'Overview',\n",
       " 'Oxnard.pdf',\n",
       " 'Palmdale.pdf',\n",
       " 'Palos Verdes.pdf',\n",
       " 'Paramount.pdf',\n",
       " 'Pasadena.pdf',\n",
       " 'Petaluma.pdf',\n",
       " 'Pico Rivera.pdf',\n",
       " 'Point Arena.pdf',\n",
       " 'Pomona.pdf',\n",
       " 'Port Hueneme.pdf',\n",
       " 'Rancho Palos Verdes.pdf',\n",
       " 'Redondo Beach.pdf',\n",
       " 'Rohnert Park.pdf',\n",
       " 'Rolling Hills Estates.pdf',\n",
       " 'Rolling Hills.pdf',\n",
       " 'Rosemead.pdf',\n",
       " 'San Dimas.pdf',\n",
       " 'San Fernando.pdf',\n",
       " 'San Gabriel.pdf',\n",
       " 'San Marino.pdf',\n",
       " 'Santa Clarita.pdf',\n",
       " 'Santa Fe Springs.pdf',\n",
       " 'Santa Monica.pdf',\n",
       " 'Santa Paula.pdf',\n",
       " 'Santa Rosa.pdf',\n",
       " 'Sebastopol.pdf',\n",
       " 'Sierra Madre.pdf',\n",
       " 'Signal Hill.pdf',\n",
       " 'Simi Valley.pdf',\n",
       " 'Sonoma.pdf',\n",
       " 'South El Monte.pdf',\n",
       " 'South Gate.pdf',\n",
       " 'South Pasadena.pdf',\n",
       " 'Temple City.pdf',\n",
       " 'Thousand Oaks.pdf',\n",
       " 'Torrance.pdf',\n",
       " 'Ukiah.pdf',\n",
       " 'Ventura.pdf',\n",
       " 'Vernon.pdf',\n",
       " 'Walnut.pdf',\n",
       " 'West Covina.pdf',\n",
       " 'West Hollywood.pdf',\n",
       " 'Westlake Village.pdf',\n",
       " 'Whittier.pdf',\n",
       " 'Willits.pdf',\n",
       " 'Windsor.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading plans\n",
    "planlist = os.listdir('C:/Users/melod/Documents/data science/Food-Systems-Policy-Research/Food Systems and General Plans/General Plans')\n",
    "planlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883368e-4edc-4509-9ba1-8f17f76f68de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Agoura Hills\n",
      "Finished Alhambra\n",
      "Finished Arcadia\n",
      "Finished Artesia\n",
      "Finished Avalon\n",
      "Finished Azusa\n",
      "Finished Baldwin Park\n",
      "Finished Bell Gardens\n",
      "Finished Bell\n",
      "Finished Bellflower\n",
      "Finished Beverly Hills\n",
      "Finished Bradbury\n",
      "Finished Burbank\n",
      "Finished Calabasas\n",
      "Finished Camarillo\n",
      "Finished Carson\n",
      "Finished Cerritos\n",
      "Finished Claremont\n",
      "Finished Clearlake\n",
      "Finished Commerce\n",
      "Finished Compton\n",
      "Finished Cotati\n",
      "Finished Covina\n",
      "Finished Cudahy\n",
      "Finished Culver City\n",
      "Finished Diamond Bar\n",
      "Finished Downey\n",
      "Finished Duarte\n",
      "Finished El Monte\n",
      "Finished El Segundo\n",
      "Finished Filmore\n",
      "Finished Fort Bragg\n",
      "Finished Glendale\n",
      "Finished Hawaiian Gardens\n",
      "Finished Hawthorne\n",
      "Finished Healdsburg\n",
      "Finished Hermosa Beach\n",
      "Finished Hidden Hills\n",
      "Finished Huntington Park\n",
      "Finished Industry\n",
      "Finished Inglewood\n",
      "Finished Irwindale\n",
      "Finished La Canada Flintridge\n",
      "Finished La Habra Heights\n",
      "Finished La Mirada\n",
      "Finished La Puente\n",
      "Finished La Verne\n",
      "Finished Lakeport\n",
      "Finished Lancaster\n",
      "Finished Lawndale\n",
      "Finished Lomita\n",
      "Finished Long Beach\n",
      "Finished Lynwood\n",
      "Finished Malibu\n",
      "Finished Manhattan Beach\n",
      "Finished Maywood\n",
      "Finished Monrovia\n",
      "Finished Montebello\n",
      "Finished Monterey Park\n",
      "Finished Moorpark\n",
      "Finished Norwalk\n",
      "Finished Ojai\n",
      "Finished Oxnard\n",
      "Finished Palmdale\n",
      "Finished Palos Verdes\n",
      "Finished Paramount\n",
      "Finished Pasadena\n",
      "Finished Petaluma\n",
      "Finished Pico Rivera\n",
      "Finished Point Arena\n"
     ]
    }
   ],
   "source": [
    "# extracting text\n",
    "\n",
    "# function\n",
    "def readPDF(planname):\n",
    "    txt = extract_text('C:/Users/melod/Documents/data science/Food-Systems-Policy-Research/Food Systems and General Plans/General Plans/'+planname)\n",
    "    \n",
    "    # remove punctuation, numbers, etc.\n",
    "    txt = re.sub(r\"[^A-z\\s]\", \"\", txt)\n",
    "    # remove whitepace\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt) \n",
    "    \n",
    "    # cleaning up name\n",
    "    planname = planname.split(\".\")[0]\n",
    "    \n",
    "    # insert muni(planname) to the beginning of plan: creating indexable muni ID within each plan string\n",
    "    txt = planname+\", \"+txt\n",
    "    \n",
    "    print('Finished {}'.format(planname))\n",
    "    return txt\n",
    "\n",
    "# read in all pdf files\n",
    "suffixes = ('.pdf', '.PDF')\n",
    "genplan = [readPDF(pn) for pn in planlist if pn.endswith(suffixes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572be5a-e2f7-4c34-9ff6-b6b48e8ca284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of plans\n",
    "genplan.to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a1adba1-1cf3-42f1-b9dc-19aa98738f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish list of stopwords to exclude\n",
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed35203-cdb1-40cc-adc5-82ad631527ef",
   "metadata": {},
   "source": [
    "#### Keyword Count (RAW TEXT VERSION: Space positions)\n",
    "\n",
    "Loop to return only count of select keywords for each plan in genplan and store in stacked df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "197ee896-5027-43ee-8b63-4108e0a9a58e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('food', 'Alhambra General Plan'): [28264,\n",
       "  92775,\n",
       "  102247,\n",
       "  107926,\n",
       "  109065,\n",
       "  113179,\n",
       "  178165,\n",
       "  198174,\n",
       "  198866,\n",
       "  209796,\n",
       "  209884,\n",
       "  210207],\n",
       " ('agriculture', 'Alhambra General Plan'): [144819],\n",
       " ('garden', 'Alhambra General Plan'): [37125,\n",
       "  39045,\n",
       "  65339,\n",
       "  97617,\n",
       "  97662,\n",
       "  97853,\n",
       "  98611,\n",
       "  102899,\n",
       "  116871,\n",
       "  118153,\n",
       "  166246,\n",
       "  166449,\n",
       "  210144],\n",
       " ('farm', 'Alhambra General Plan'): [11630],\n",
       " ('fruit', 'Alhambra General Plan'): [101651],\n",
       " ('vegetable', 'Alhambra General Plan'): [98417, 101658, 210134],\n",
       " ('animal', 'Alhambra General Plan'): [124235],\n",
       " ('soil', 'Alhambra General Plan'): [18274,\n",
       "  27578,\n",
       "  121375,\n",
       "  125060,\n",
       "  178858,\n",
       "  178959,\n",
       "  178977,\n",
       "  179005,\n",
       "  179188,\n",
       "  180326,\n",
       "  180400,\n",
       "  180684,\n",
       "  180861,\n",
       "  181002,\n",
       "  182443,\n",
       "  186816,\n",
       "  203699,\n",
       "  203796],\n",
       " ('remediation', 'Alhambra General Plan'): [],\n",
       " ('contaminate', 'Alhambra General Plan'): [],\n",
       " ('carbon', 'Alhambra General Plan'): [134877, 135016, 137259],\n",
       " ('sustainability', 'Alhambra General Plan'): [140202],\n",
       " ('climate', 'Alhambra General Plan'): [19215,\n",
       "  19282,\n",
       "  25232,\n",
       "  25435,\n",
       "  28188,\n",
       "  33765,\n",
       "  121436,\n",
       "  121768,\n",
       "  125225,\n",
       "  125404,\n",
       "  134458,\n",
       "  134596,\n",
       "  134781,\n",
       "  137219,\n",
       "  137295,\n",
       "  137388,\n",
       "  138817,\n",
       "  138949,\n",
       "  139884,\n",
       "  149838,\n",
       "  151106,\n",
       "  152013,\n",
       "  152585,\n",
       "  199507,\n",
       "  199858,\n",
       "  200014,\n",
       "  200077,\n",
       "  200141,\n",
       "  200369,\n",
       "  201329,\n",
       "  201949,\n",
       "  201978,\n",
       "  202526,\n",
       "  208938,\n",
       "  209127,\n",
       "  209225,\n",
       "  209356,\n",
       "  209394,\n",
       "  209524],\n",
       " ('environment', 'Alhambra General Plan'): [8221,\n",
       "  10929,\n",
       "  11470,\n",
       "  14178,\n",
       "  14621,\n",
       "  19776,\n",
       "  20738,\n",
       "  20778,\n",
       "  21326,\n",
       "  21812,\n",
       "  22052,\n",
       "  24737,\n",
       "  25744,\n",
       "  26417,\n",
       "  26647,\n",
       "  41684,\n",
       "  41803,\n",
       "  45651,\n",
       "  45851,\n",
       "  46889,\n",
       "  47320,\n",
       "  48121,\n",
       "  55551,\n",
       "  55854,\n",
       "  58953,\n",
       "  63155,\n",
       "  67482,\n",
       "  67736,\n",
       "  70757,\n",
       "  70935,\n",
       "  86169,\n",
       "  100537,\n",
       "  106948,\n",
       "  107130,\n",
       "  107420,\n",
       "  107515,\n",
       "  108362,\n",
       "  108696,\n",
       "  109003,\n",
       "  109158,\n",
       "  109331,\n",
       "  109450,\n",
       "  109559,\n",
       "  110357,\n",
       "  110726,\n",
       "  111842,\n",
       "  112038,\n",
       "  114223,\n",
       "  116722,\n",
       "  119187,\n",
       "  126717,\n",
       "  126917,\n",
       "  134717,\n",
       "  135892,\n",
       "  138490,\n",
       "  140000,\n",
       "  140620,\n",
       "  140863,\n",
       "  141844,\n",
       "  171260,\n",
       "  173498,\n",
       "  176398,\n",
       "  184511,\n",
       "  185392,\n",
       "  185658,\n",
       "  186077,\n",
       "  193455,\n",
       "  194986,\n",
       "  195536,\n",
       "  198357,\n",
       "  198437,\n",
       "  203278,\n",
       "  207775],\n",
       " ('health', 'Alhambra General Plan'): [10960,\n",
       "  14032,\n",
       "  18160,\n",
       "  18633,\n",
       "  19738,\n",
       "  21399,\n",
       "  25803,\n",
       "  28212,\n",
       "  40590,\n",
       "  43948,\n",
       "  45865,\n",
       "  106054,\n",
       "  106379,\n",
       "  107488,\n",
       "  107733,\n",
       "  110274,\n",
       "  111331,\n",
       "  111701,\n",
       "  119803,\n",
       "  119847,\n",
       "  121243,\n",
       "  130702,\n",
       "  135917,\n",
       "  154537,\n",
       "  177209,\n",
       "  177410,\n",
       "  178039,\n",
       "  178157,\n",
       "  178200,\n",
       "  178313,\n",
       "  184497,\n",
       "  185643,\n",
       "  187408,\n",
       "  189043,\n",
       "  198004,\n",
       "  198142,\n",
       "  198321,\n",
       "  198407,\n",
       "  198496,\n",
       "  198544,\n",
       "  198599,\n",
       "  198797,\n",
       "  198858,\n",
       "  198875,\n",
       "  198903,\n",
       "  199311,\n",
       "  199576,\n",
       "  200767,\n",
       "  200852,\n",
       "  202562,\n",
       "  203340,\n",
       "  205958,\n",
       "  208838,\n",
       "  209010,\n",
       "  209607,\n",
       "  209646,\n",
       "  209725,\n",
       "  209938,\n",
       "  210029,\n",
       "  210173],\n",
       " ('safety', 'Alhambra General Plan'): [8351,\n",
       "  10971,\n",
       "  14039,\n",
       "  18171,\n",
       "  25814,\n",
       "  76324,\n",
       "  85244,\n",
       "  88341,\n",
       "  95129,\n",
       "  105411,\n",
       "  105481,\n",
       "  106610,\n",
       "  154544,\n",
       "  155388,\n",
       "  157620,\n",
       "  173210,\n",
       "  173825,\n",
       "  177216,\n",
       "  177421,\n",
       "  177656,\n",
       "  177832,\n",
       "  177929,\n",
       "  178503,\n",
       "  178542,\n",
       "  191066,\n",
       "  191595,\n",
       "  193803,\n",
       "  200048,\n",
       "  200778,\n",
       "  203351,\n",
       "  205662,\n",
       "  205969],\n",
       " ('justice', 'Alhambra General Plan'): [19790,\n",
       "  21340,\n",
       "  107144,\n",
       "  107642,\n",
       "  108376,\n",
       "  108553,\n",
       "  108710,\n",
       "  109345,\n",
       "  110740,\n",
       "  111856,\n",
       "  116736],\n",
       " ('food', 'Cerritos'): [643873],\n",
       " ('agriculture', 'Cerritos'): [25638, 93314, 298724, 298867],\n",
       " ('garden', 'Cerritos'): [93304, 105099, 299013, 678935, 679034, 772546],\n",
       " ('farm', 'Cerritos'): [184898,\n",
       "  185039,\n",
       "  277489,\n",
       "  298805,\n",
       "  298942,\n",
       "  299175,\n",
       "  368430,\n",
       "  368918,\n",
       "  369002,\n",
       "  434261,\n",
       "  447461,\n",
       "  513504,\n",
       "  530287],\n",
       " ('fruit', 'Cerritos'): [],\n",
       " ('vegetable', 'Cerritos'): [],\n",
       " ('animal', 'Cerritos'): [329759, 603165, 658103, 690313, 690515],\n",
       " ('soil', 'Cerritos'): [632613,\n",
       "  632893,\n",
       "  635352,\n",
       "  635983,\n",
       "  640696,\n",
       "  640783,\n",
       "  640799,\n",
       "  640843,\n",
       "  640926,\n",
       "  641166,\n",
       "  641271,\n",
       "  672909,\n",
       "  673320,\n",
       "  687393],\n",
       " ('remediation', 'Cerritos'): [404025],\n",
       " ('contaminate', 'Cerritos'): [681388],\n",
       " ('carbon', 'Cerritos'): [737801, 740221, 747988, 750622, 752003],\n",
       " ('sustainability', 'Cerritos'): [431875, 795599, 804479],\n",
       " ('climate', 'Cerritos'): [75522,\n",
       "  133730,\n",
       "  428227,\n",
       "  516000,\n",
       "  736384,\n",
       "  737106,\n",
       "  738300,\n",
       "  738560],\n",
       " ('environment', 'Cerritos'): [2710,\n",
       "  7204,\n",
       "  10012,\n",
       "  10236,\n",
       "  14697,\n",
       "  19191,\n",
       "  21999,\n",
       "  22223,\n",
       "  47840,\n",
       "  55353,\n",
       "  57432,\n",
       "  58494,\n",
       "  58681,\n",
       "  61587,\n",
       "  62444,\n",
       "  62570,\n",
       "  63600,\n",
       "  63770,\n",
       "  64775,\n",
       "  64863,\n",
       "  65879,\n",
       "  66781,\n",
       "  67858,\n",
       "  67933,\n",
       "  73574,\n",
       "  74437,\n",
       "  74676,\n",
       "  76631,\n",
       "  83848,\n",
       "  85105,\n",
       "  89656,\n",
       "  91162,\n",
       "  98065,\n",
       "  100126,\n",
       "  109534,\n",
       "  112089,\n",
       "  118399,\n",
       "  123754,\n",
       "  125258,\n",
       "  129916,\n",
       "  131151,\n",
       "  131550,\n",
       "  131843,\n",
       "  132107,\n",
       "  132214,\n",
       "  132601,\n",
       "  132656,\n",
       "  132833,\n",
       "  133100,\n",
       "  137379,\n",
       "  137524,\n",
       "  138034,\n",
       "  138587,\n",
       "  138808,\n",
       "  139891,\n",
       "  140579,\n",
       "  141686,\n",
       "  141777,\n",
       "  141999,\n",
       "  143626,\n",
       "  143810,\n",
       "  144466,\n",
       "  144628,\n",
       "  144675,\n",
       "  144741,\n",
       "  144902,\n",
       "  145159,\n",
       "  145436,\n",
       "  149027,\n",
       "  149350,\n",
       "  149598,\n",
       "  149646,\n",
       "  150023,\n",
       "  151833,\n",
       "  153510,\n",
       "  155513,\n",
       "  163807,\n",
       "  163976,\n",
       "  164632,\n",
       "  164764,\n",
       "  164904,\n",
       "  165464,\n",
       "  165519,\n",
       "  167409,\n",
       "  167706,\n",
       "  167880,\n",
       "  188910,\n",
       "  206575,\n",
       "  206598,\n",
       "  207165,\n",
       "  289482,\n",
       "  289552,\n",
       "  289662,\n",
       "  310155,\n",
       "  310243,\n",
       "  313723,\n",
       "  318805,\n",
       "  320139,\n",
       "  327686,\n",
       "  351733,\n",
       "  351946,\n",
       "  352092,\n",
       "  352450,\n",
       "  352517,\n",
       "  391802,\n",
       "  392052,\n",
       "  431808,\n",
       "  432066,\n",
       "  432490,\n",
       "  434217,\n",
       "  435033,\n",
       "  436588,\n",
       "  437007,\n",
       "  527316,\n",
       "  573242,\n",
       "  573403,\n",
       "  573558,\n",
       "  573981,\n",
       "  577300,\n",
       "  578988,\n",
       "  579112,\n",
       "  579182,\n",
       "  579213,\n",
       "  599495,\n",
       "  611264,\n",
       "  648302,\n",
       "  681476,\n",
       "  695419,\n",
       "  695482,\n",
       "  695513,\n",
       "  696006,\n",
       "  696184,\n",
       "  696350,\n",
       "  697839,\n",
       "  729271,\n",
       "  730373,\n",
       "  732830,\n",
       "  733762,\n",
       "  734568,\n",
       "  758547,\n",
       "  760192,\n",
       "  761127,\n",
       "  761447,\n",
       "  766942,\n",
       "  767662,\n",
       "  770521,\n",
       "  770581,\n",
       "  772221,\n",
       "  772466,\n",
       "  772688,\n",
       "  778646,\n",
       "  782460,\n",
       "  784806,\n",
       "  788387,\n",
       "  788528,\n",
       "  790422,\n",
       "  811325],\n",
       " ('health', 'Cerritos'): [9547,\n",
       "  21534,\n",
       "  55461,\n",
       "  78841,\n",
       "  89640,\n",
       "  91146,\n",
       "  92988,\n",
       "  101831,\n",
       "  159220,\n",
       "  194548,\n",
       "  228546,\n",
       "  228760,\n",
       "  297382,\n",
       "  320174,\n",
       "  361181,\n",
       "  361209,\n",
       "  364055,\n",
       "  364436,\n",
       "  366499,\n",
       "  377407,\n",
       "  384570,\n",
       "  394247,\n",
       "  396690,\n",
       "  419165,\n",
       "  422983,\n",
       "  423119,\n",
       "  432312,\n",
       "  445053,\n",
       "  485393,\n",
       "  491877,\n",
       "  514543,\n",
       "  537773,\n",
       "  565803,\n",
       "  578669,\n",
       "  660856,\n",
       "  669846,\n",
       "  670002,\n",
       "  681459,\n",
       "  681551,\n",
       "  686830,\n",
       "  695377,\n",
       "  695405,\n",
       "  700942,\n",
       "  702025,\n",
       "  707801,\n",
       "  708436,\n",
       "  712909,\n",
       "  733837,\n",
       "  735256,\n",
       "  745624,\n",
       "  745791,\n",
       "  746731,\n",
       "  748576,\n",
       "  749864,\n",
       "  752173,\n",
       "  805343],\n",
       " ('safety', 'Cerritos'): [55472,\n",
       "  78848,\n",
       "  93899,\n",
       "  104197,\n",
       "  107545,\n",
       "  107845,\n",
       "  108043,\n",
       "  134945,\n",
       "  138195,\n",
       "  138878,\n",
       "  157657,\n",
       "  157762,\n",
       "  157933,\n",
       "  158559,\n",
       "  159227,\n",
       "  194559,\n",
       "  228553,\n",
       "  228767,\n",
       "  258097,\n",
       "  359449,\n",
       "  367496,\n",
       "  384581,\n",
       "  394254,\n",
       "  396701,\n",
       "  419176,\n",
       "  422994,\n",
       "  423130,\n",
       "  437041,\n",
       "  483142,\n",
       "  485400,\n",
       "  491888,\n",
       "  512056,\n",
       "  514937,\n",
       "  527279,\n",
       "  535799,\n",
       "  628111,\n",
       "  644570,\n",
       "  655690,\n",
       "  656081,\n",
       "  658729,\n",
       "  659049,\n",
       "  662642,\n",
       "  663589,\n",
       "  664507,\n",
       "  667284,\n",
       "  668397,\n",
       "  668546,\n",
       "  669283,\n",
       "  669567,\n",
       "  669853,\n",
       "  670009,\n",
       "  681558,\n",
       "  686837,\n",
       "  693650,\n",
       "  696097,\n",
       "  700953,\n",
       "  707821,\n",
       "  708447,\n",
       "  766511,\n",
       "  805354,\n",
       "  808497],\n",
       " ('justice', 'Cerritos'): [],\n",
       " ('food', 'City of Commerce 2020 General Plan'): [8724, 98911, 575597, 621711],\n",
       " ('agriculture', 'City of Commerce 2020 General Plan'): [120021],\n",
       " ('garden', 'City of Commerce 2020 General Plan'): [],\n",
       " ('farm', 'City of Commerce 2020 General Plan'): [193528,\n",
       "  318124,\n",
       "  318336,\n",
       "  318611],\n",
       " ('fruit', 'City of Commerce 2020 General Plan'): [],\n",
       " ('vegetable', 'City of Commerce 2020 General Plan'): [],\n",
       " ('animal', 'City of Commerce 2020 General Plan'): [416624, 437928, 489387],\n",
       " ('soil', 'City of Commerce 2020 General Plan'): [51626,\n",
       "  51649,\n",
       "  342596,\n",
       "  417978,\n",
       "  471374,\n",
       "  471397,\n",
       "  485919,\n",
       "  494988,\n",
       "  498213,\n",
       "  604514,\n",
       "  604537,\n",
       "  660155,\n",
       "  660178],\n",
       " ('remediation', 'City of Commerce 2020 General Plan'): [469684],\n",
       " ('contaminate', 'City of Commerce 2020 General Plan'): [469650,\n",
       "  498200,\n",
       "  539087],\n",
       " ('carbon', 'City of Commerce 2020 General Plan'): [506717,\n",
       "  524068,\n",
       "  526908,\n",
       "  527017,\n",
       "  528569,\n",
       "  533614,\n",
       "  536547],\n",
       " ('sustainability', 'City of Commerce 2020 General Plan'): [244741],\n",
       " ('climate', 'City of Commerce 2020 General Plan'): [],\n",
       " ('environment', 'City of Commerce 2020 General Plan'): [14314,\n",
       "  14959,\n",
       "  15284,\n",
       "  26803,\n",
       "  37004,\n",
       "  46465,\n",
       "  48715,\n",
       "  52684,\n",
       "  56877,\n",
       "  59619,\n",
       "  60450,\n",
       "  70025,\n",
       "  72540,\n",
       "  77237,\n",
       "  80784,\n",
       "  83126,\n",
       "  83210,\n",
       "  83696,\n",
       "  84002,\n",
       "  84151,\n",
       "  84229,\n",
       "  84326,\n",
       "  102711,\n",
       "  105134,\n",
       "  105554,\n",
       "  106403,\n",
       "  106695,\n",
       "  106788,\n",
       "  106984,\n",
       "  137036,\n",
       "  163154,\n",
       "  165042,\n",
       "  165325,\n",
       "  165385,\n",
       "  165588,\n",
       "  198025,\n",
       "  200307,\n",
       "  200958,\n",
       "  201243,\n",
       "  327364,\n",
       "  328098,\n",
       "  346612,\n",
       "  346825,\n",
       "  372427,\n",
       "  389650,\n",
       "  389805,\n",
       "  390094,\n",
       "  424062,\n",
       "  437488,\n",
       "  437773,\n",
       "  437827,\n",
       "  441979,\n",
       "  453754,\n",
       "  456901,\n",
       "  460350,\n",
       "  465074,\n",
       "  466909,\n",
       "  472432,\n",
       "  476762,\n",
       "  479510,\n",
       "  483319,\n",
       "  483602,\n",
       "  483680,\n",
       "  483918,\n",
       "  485610,\n",
       "  487606,\n",
       "  491399,\n",
       "  491770,\n",
       "  491945,\n",
       "  506792,\n",
       "  509991,\n",
       "  519330,\n",
       "  521971,\n",
       "  522263,\n",
       "  522356,\n",
       "  522552,\n",
       "  524143,\n",
       "  531749,\n",
       "  550324,\n",
       "  550616,\n",
       "  550709,\n",
       "  550905,\n",
       "  579491,\n",
       "  601280,\n",
       "  602251,\n",
       "  605614,\n",
       "  609960,\n",
       "  626714,\n",
       "  655618,\n",
       "  657294,\n",
       "  661676,\n",
       "  666992],\n",
       " ('health', 'City of Commerce 2020 General Plan'): [6526,\n",
       "  29758,\n",
       "  32722,\n",
       "  37801,\n",
       "  50724,\n",
       "  51182,\n",
       "  58743,\n",
       "  59601,\n",
       "  60512,\n",
       "  60862,\n",
       "  70083,\n",
       "  70208,\n",
       "  90923,\n",
       "  95909,\n",
       "  211866,\n",
       "  214992,\n",
       "  223467,\n",
       "  243950,\n",
       "  259568,\n",
       "  317993,\n",
       "  322309,\n",
       "  332161,\n",
       "  375931,\n",
       "  379787,\n",
       "  388974,\n",
       "  389349,\n",
       "  398957,\n",
       "  400760,\n",
       "  404420,\n",
       "  420497,\n",
       "  420669,\n",
       "  425332,\n",
       "  453217,\n",
       "  457186,\n",
       "  459779,\n",
       "  460219,\n",
       "  460364,\n",
       "  470385,\n",
       "  470930,\n",
       "  478634,\n",
       "  479492,\n",
       "  487324,\n",
       "  492286,\n",
       "  493904,\n",
       "  503710,\n",
       "  506534,\n",
       "  506626,\n",
       "  506825,\n",
       "  510053,\n",
       "  510322,\n",
       "  519388,\n",
       "  519513,\n",
       "  523885,\n",
       "  523977,\n",
       "  524176,\n",
       "  527780,\n",
       "  528197,\n",
       "  535376,\n",
       "  535873,\n",
       "  545435,\n",
       "  575086,\n",
       "  589705,\n",
       "  600376,\n",
       "  601262,\n",
       "  604205,\n",
       "  610022,\n",
       "  610291,\n",
       "  621081,\n",
       "  640268,\n",
       "  654438,\n",
       "  655600,\n",
       "  659790,\n",
       "  667054,\n",
       "  667323],\n",
       " ('safety', 'City of Commerce 2020 General Plan'): [4298,\n",
       "  4402,\n",
       "  22916,\n",
       "  47147,\n",
       "  47283,\n",
       "  50735,\n",
       "  53839,\n",
       "  58534,\n",
       "  58647,\n",
       "  58750,\n",
       "  58928,\n",
       "  59608,\n",
       "  74423,\n",
       "  76033,\n",
       "  76137,\n",
       "  77879,\n",
       "  158189,\n",
       "  215414,\n",
       "  223478,\n",
       "  322320,\n",
       "  332168,\n",
       "  404431,\n",
       "  440092,\n",
       "  453228,\n",
       "  453439,\n",
       "  453548,\n",
       "  453791,\n",
       "  455207,\n",
       "  455360,\n",
       "  455400,\n",
       "  455469,\n",
       "  457197,\n",
       "  459790,\n",
       "  460230,\n",
       "  461878,\n",
       "  468252,\n",
       "  468388,\n",
       "  469283,\n",
       "  470396,\n",
       "  473713,\n",
       "  478425,\n",
       "  478538,\n",
       "  478641,\n",
       "  478819,\n",
       "  479499,\n",
       "  481408,\n",
       "  481546,\n",
       "  482211,\n",
       "  483644,\n",
       "  484648,\n",
       "  487331,\n",
       "  492247,\n",
       "  492369,\n",
       "  493915,\n",
       "  506673,\n",
       "  524024,\n",
       "  545446,\n",
       "  551043,\n",
       "  551181,\n",
       "  551674,\n",
       "  552611,\n",
       "  558743,\n",
       "  560254,\n",
       "  584442,\n",
       "  592008,\n",
       "  592086,\n",
       "  600383,\n",
       "  600568,\n",
       "  601269,\n",
       "  602981,\n",
       "  603124,\n",
       "  604216,\n",
       "  606203,\n",
       "  633183,\n",
       "  654445,\n",
       "  654691,\n",
       "  655607,\n",
       "  658320,\n",
       "  658494,\n",
       "  659801,\n",
       "  662382],\n",
       " ('justice', 'City of Commerce 2020 General Plan'): [14973,\n",
       "  15298,\n",
       "  26817,\n",
       "  60464,\n",
       "  72554,\n",
       "  83140,\n",
       "  83224,\n",
       "  83710,\n",
       "  84016,\n",
       "  84165,\n",
       "  84243,\n",
       "  84340,\n",
       "  102725,\n",
       "  105568,\n",
       "  163168,\n",
       "  198039,\n",
       "  200972,\n",
       "  201257,\n",
       "  510005,\n",
       "  609974,\n",
       "  667006],\n",
       " ('food', 'Vernon_General_Plan'): [5954, 7710, 218443, 291871],\n",
       " ('agriculture', 'Vernon_General_Plan'): [230635],\n",
       " ('garden', 'Vernon_General_Plan'): [],\n",
       " ('farm', 'Vernon_General_Plan'): [4587,\n",
       "  4800,\n",
       "  5042,\n",
       "  104007,\n",
       "  110193,\n",
       "  110261,\n",
       "  113939,\n",
       "  292518],\n",
       " ('fruit', 'Vernon_General_Plan'): [],\n",
       " ('vegetable', 'Vernon_General_Plan'): [277829],\n",
       " ('animal', 'Vernon_General_Plan'): [34241,\n",
       "  146755,\n",
       "  147041,\n",
       "  164757,\n",
       "  290685,\n",
       "  290784,\n",
       "  290872,\n",
       "  291852],\n",
       " ('soil', 'Vernon_General_Plan'): [102164,\n",
       "  137971,\n",
       "  143263,\n",
       "  143489,\n",
       "  143989,\n",
       "  144221,\n",
       "  165213,\n",
       "  201549,\n",
       "  201787,\n",
       "  201947,\n",
       "  283867,\n",
       "  285507,\n",
       "  290526],\n",
       " ('remediation', 'Vernon_General_Plan'): [138057, 138428],\n",
       " ('contaminate', 'Vernon_General_Plan'): [144205, 262259],\n",
       " ('carbon', 'Vernon_General_Plan'): [],\n",
       " ('sustainability', 'Vernon_General_Plan'): [220715],\n",
       " ('climate', 'Vernon_General_Plan'): [225182],\n",
       " ('environment', 'Vernon_General_Plan'): [7253,\n",
       "  8021,\n",
       "  8279,\n",
       "  15372,\n",
       "  15421,\n",
       "  15463,\n",
       "  15580,\n",
       "  19793,\n",
       "  86169,\n",
       "  89540,\n",
       "  93848,\n",
       "  106172,\n",
       "  111178,\n",
       "  112082,\n",
       "  112223,\n",
       "  112435,\n",
       "  115473,\n",
       "  116058,\n",
       "  136879,\n",
       "  138858,\n",
       "  154524,\n",
       "  158047,\n",
       "  161294,\n",
       "  178492,\n",
       "  183314,\n",
       "  185742,\n",
       "  197157,\n",
       "  197207,\n",
       "  215385,\n",
       "  216037,\n",
       "  225227,\n",
       "  235173,\n",
       "  235989,\n",
       "  241915,\n",
       "  242062,\n",
       "  252564,\n",
       "  253807,\n",
       "  270967,\n",
       "  278108,\n",
       "  280188,\n",
       "  283044,\n",
       "  290498,\n",
       "  290620],\n",
       " ('health', 'Vernon_General_Plan'): [44424,\n",
       "  83049,\n",
       "  89413,\n",
       "  89838,\n",
       "  93866,\n",
       "  112887,\n",
       "  144455,\n",
       "  176903,\n",
       "  187354,\n",
       "  187748,\n",
       "  210556,\n",
       "  215375,\n",
       "  223252,\n",
       "  233309,\n",
       "  234994,\n",
       "  235455,\n",
       "  238277,\n",
       "  240561,\n",
       "  270952,\n",
       "  288374,\n",
       "  290605],\n",
       " ('safety', 'Vernon_General_Plan'): [9510,\n",
       "  44435,\n",
       "  49845,\n",
       "  83060,\n",
       "  86158,\n",
       "  89424,\n",
       "  126834,\n",
       "  138490,\n",
       "  149292,\n",
       "  161308,\n",
       "  176914,\n",
       "  187361,\n",
       "  187759,\n",
       "  187914,\n",
       "  196680,\n",
       "  197075,\n",
       "  197109,\n",
       "  197631,\n",
       "  197721,\n",
       "  199656,\n",
       "  200193,\n",
       "  200211,\n",
       "  200587,\n",
       "  202782,\n",
       "  206760,\n",
       "  210179,\n",
       "  210504,\n",
       "  210563,\n",
       "  210888,\n",
       "  212086,\n",
       "  213123,\n",
       "  213584,\n",
       "  214230,\n",
       "  214302,\n",
       "  240550,\n",
       "  266639,\n",
       "  283679],\n",
       " ('justice', 'Vernon_General_Plan'): [112237]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving positions of keywords in raw plan text to a dictionary: all plans\n",
    "textdictall = {}\n",
    "plan_cols = []\n",
    "\n",
    "keywords = ['food', 'agriculture', 'garden', 'farm', 'fruit', 'vegetable', 'animal', 'soil', \n",
    "            'remediation', 'contaminate', 'sustainability', 'climate', 'environment', \n",
    "            'health', 'safety', 'justice']\n",
    "\n",
    "for plan in genplan:\n",
    "    for key in keywords:\n",
    "        # identify text positions of keyword mentions\n",
    "        textpositionsall = [i for i in range(len(plan)) if plan.startswith(key, i)] # adapted: https://www.geeksforgeeks.org/python-all-occurrences-of-substring-in-string/#\n",
    "        # save list of positions for each keyword in a dictionary\n",
    "        textdictall[key] = textpositionsall\n",
    "\n",
    "    textdict = {}\n",
    "\n",
    "    # generate counts for each keyword by summing the number of positions in its position list\n",
    "    for keyword in textdictall:\n",
    "        for position in keyword:\n",
    "            # store counts in dictionary\n",
    "            textdict[keyword] = len(textdictall[keyword])\n",
    "\n",
    "        # turn dicts into dfs\n",
    "        keywordscount = pd.DataFrame.from_dict(textdict, orient='index', columns=[plan.split(\", \")[0]]) # muni plan name as series name\n",
    "    \n",
    "    # add each municipality column generated to a single dataframe\n",
    "    plan_cols.append(keywordscount)\n",
    "    munikeys = pd.concat(plan_cols, axis = 1)\n",
    "\n",
    "# inspect/show\n",
    "munikeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c095ad9e-5f8c-4e9e-9b72-99edba0a1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorienting table the keyword counts so they can be joined to other gdfs\n",
    "munikeyst = munikeys.transpose()\n",
    "\n",
    "# naming index column\n",
    "munikeyst.index.name = \"City\"\n",
    "\n",
    "# show\n",
    "munikeyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e18c61-efa5-4431-98b9-63a18fe9f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving keyword counts table\n",
    "munikeyst.to_csv(\"Plan Keyword Counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7899c6-7e6a-4f15-83e7-3aa14e1fd16d",
   "metadata": {},
   "source": [
    "Now to visualize which cities potentially include community food production in their long-range land use planning  goals and strategies, the keyword count table will be merged with the table including municipal boundaries and preliminary information about the density of urban agriculture sites and facilities that produce toxic waste in a given municipality (created in the \"LAC Map\" notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3f3d26b7-74df-4f23-98d1-9f5c5fae04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining keyword counts table to table with geocoded municipalities with preliminary UA site and Toxic Release densities\n",
    "\n",
    "# loading file\n",
    "LAC = gpd.read_file('C:/Users/melod/Documents/data science/Food-Systems-Policy-Research/Food Systems and General Plans/LACfinal.json')\n",
    "\n",
    "# resetting index for keyword df\n",
    "keywordcounts1 = keywordcounts.set_index('municipality')\n",
    "\n",
    "# joining\n",
    "LACmunikeys = LAC.join(munikeyst, how= 'left', rsuffix = '_counties')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "440da546-dd48-4a7b-85f1-92bc21c1a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual barplots (color-coded by north or south)\n",
    "\n",
    "# municipalities = []\n",
    "\n",
    "#fig ax, = plt.subplots(size= (8, 12))\n",
    "\n",
    "#keywordscount.plot (ax=ax, column = 'keyword', size = 'counts', legend = True, legendkwds = {orientation: ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "901a69b6-17df-495b-89df-a405958813f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated: by keyword clusters: frequency north vs. south\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a4295-47d2-430a-a78b-3419d0e809cd",
   "metadata": {},
   "source": [
    "### (B) Proximity of Keywords in Plans: How is Food and Urban Agriculture Being Talked About?\n",
    "\n",
    "This next exercise aims to make inferences about how topics like food are being talked about based on the proximity (distance of words) between keywords (e.g. \"food\" and \"soil\" or \"food\" and \"safety\"). \n",
    "\n",
    "For each plan, keyword set pairs will be selected and matrix math will be used to compare the distance between the individual words in those keyword set pairs. The goal is to produce 28 matrices for word pairs for each plan for the following:\n",
    "\n",
    "    Set 1: FOOD, AGRICULTURE, GARDEN, FARM\n",
    "    Set 2: safety, soil, sustainability, climate, environment, remediation, justice, public, residential\n",
    "    \n",
    "-- and then to identify the frequency of word pairs that are within 100 or less words of each other in each municipality/plan. This would need to be done with a cleaned wordlist instead of the raw text, which might produce approximate word count distances for unique word pairs that exclude a lot of (filler) words, but would ultimately be more meaningful than distance measured by spaces in the raw text.\n",
    "\n",
    "The hope is that this method will allow me to identify where (which municipalities) are talking about food and community agriculture in conjunction with environmental health and safety more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a6458-a336-43d4-985e-662457522396",
   "metadata": {},
   "source": [
    "Turning wordlists into arrays in order to do matrix math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "968518d0-4325-4b50-8b29-25ee42b42e13",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alhambra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(food, soil)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(food, remediation)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(food, sustainability)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(food, climate)</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(food, environment)</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(food, safety)</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(food, justice)</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(food, public)</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(food, residential)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(garden, soil)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(garden, remediation)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(garden, sustainability)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(garden, climate)</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(garden, environment)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(garden, safety)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(garden, justice)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(garden, public)</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(garden, residential)</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(farm, soil)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(farm, remediation)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(farm, sustainability)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(farm, climate)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(farm, environment)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(farm, safety)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(farm, justice)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(farm, public)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(farm, residential)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(agriculture, soil)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(agriculture, remediation)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(agriculture, sustainability)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(agriculture, climate)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(agriculture, environment)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(agriculture, safety)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(agriculture, justice)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(agriculture, public)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(agriculture, residential)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Alhambra\n",
       "(food, soil)                          1\n",
       "(food, remediation)                   1\n",
       "(food, sustainability)                0\n",
       "(food, climate)                       6\n",
       "(food, environment)                   2\n",
       "(food, safety)                       12\n",
       "(food, justice)                      13\n",
       "(food, public)                       15\n",
       "(food, residential)                   0\n",
       "(garden, soil)                        1\n",
       "(garden, remediation)                 0\n",
       "(garden, sustainability)              1\n",
       "(garden, climate)                     3\n",
       "(garden, environment)                 1\n",
       "(garden, safety)                      0\n",
       "(garden, justice)                     0\n",
       "(garden, public)                      8\n",
       "(garden, residential)                 8\n",
       "(farm, soil)                          0\n",
       "(farm, remediation)                   0\n",
       "(farm, sustainability)                0\n",
       "(farm, climate)                       0\n",
       "(farm, environment)                   1\n",
       "(farm, safety)                        1\n",
       "(farm, justice)                       0\n",
       "(farm, public)                        1\n",
       "(farm, residential)                   0\n",
       "(agriculture, soil)                   0\n",
       "(agriculture, remediation)            0\n",
       "(agriculture, sustainability)         0\n",
       "(agriculture, climate)                0\n",
       "(agriculture, environment)            0\n",
       "(agriculture, safety)                 0\n",
       "(agriculture, justice)                0\n",
       "(agriculture, public)                 0\n",
       "(agriculture, residential)            1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each plan\n",
    "keywords1 = ['food', 'garden', 'farm', 'agriculture']  \n",
    "keywords2 = ['soil', 'remediation', 'sustainability', 'climate', 'environment', 'safety', 'justice',\n",
    "            'public', 'residential']\n",
    "\n",
    "proxim = {}\n",
    "plan_col = []\n",
    "\n",
    "for plan in genplan:\n",
    "    \n",
    "    # create a wordlist for each plan\n",
    "    wordlist = [word for word in word_tokenize(plan.lower()) \n",
    "                 if word not in swords]\n",
    "    \n",
    "    # create dictionaries for keyword set 1 for each plan\n",
    "    lpostdict1 = {}\n",
    "    for key1 in keywords1:\n",
    "        # generate topic counts to populate dictionary frame indexed to topic\n",
    "        positions1 = [i for i,w in enumerate(wordlist) if w.lower() == key1]\n",
    "        lposdict1[key1] = positions1\n",
    "\n",
    "    # create dictionaries for keyword set 2 for each plan\n",
    "    lpostdict2 = {}\n",
    "    for key2 in keywords2:\n",
    "        # generate topic counts to populate dictionary frame indexed to topic\n",
    "        positions2 = [i for i,w in enumerate(wordlist) if w.lower() == key2]\n",
    "        lposdict2[key2] = positions2\n",
    "    \n",
    "    for key1, positions1 in lposdict1.items():\n",
    "        for key2, positions2 in lposdict2.items():\n",
    "\n",
    "            # create subtraction matrix for each keyword pair\n",
    "            key1_key2 = abs(np.subtract(np.array(positions1), np.array(positions2)[:,None])) # with different array shapes\n",
    "            # count occurences in each matrix where distance between keywords is <100 words + store in dictionary \n",
    "            proxim[key1, key2] = np.count_nonzero(key1_key2 < 100)\n",
    "            # convert dictionary to dataframe series/column for each municipality\n",
    "            prox = pd.DataFrame.from_dict(proxim, orient='index', columns=[plan.split(\", \")[0]])\n",
    "    \n",
    "    # add each muni column generated to a single dataframe\n",
    "    plan_col.append(prox)\n",
    "    muniprox = pd.concat(plan_col, axis=1)\n",
    "\n",
    "muniprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d47bd-4b88-4a3e-bada-e67f9ee09237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorienting table the keyword pair frequencies can be joined to other gdfs\n",
    "muniproxt = muniprox.transpose()\n",
    "muniproxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08fba3b-a34d-4a21-bb21-31fb99610db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving keyword pairs table\n",
    "#muniproxt.to_csv(\"Plan Keyword Proximities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712fcd1-2b52-4c45-ab83-cfabdb2f24c5",
   "metadata": {},
   "source": [
    "### (C) Modified Topic Modeling: How Are Municipalities Treating Food & Urban Agriculture as Policy Priority?\n",
    "\n",
    "The last exercise focused on identifying/verifying whether food and urban agriculture was being talked about in relation to a specific set of topics and keywords (health, safety, sustainability, climate). This is a variation on topic modeling that focuses more agnostically on finding the common words/topics around mentions of food and urban agriculture in general plans to gauge how food is talked about in municipalities more generally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e45110a5-4a59-406d-b8ca-1623f5dea331",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('food', 'Alhambra General Plan'): [28264,\n",
       "  92775,\n",
       "  102247,\n",
       "  107926,\n",
       "  109065,\n",
       "  113179,\n",
       "  178165,\n",
       "  198174,\n",
       "  198866,\n",
       "  209796,\n",
       "  209884,\n",
       "  210207],\n",
       " ('agriculture', 'Alhambra General Plan'): [144819],\n",
       " ('garden', 'Alhambra General Plan'): [37125,\n",
       "  39045,\n",
       "  65339,\n",
       "  97617,\n",
       "  97662,\n",
       "  97853,\n",
       "  98611,\n",
       "  102899,\n",
       "  116871,\n",
       "  118153,\n",
       "  166246,\n",
       "  166449,\n",
       "  210144],\n",
       " ('farm', 'Alhambra General Plan'): [11630],\n",
       " ('fruit', 'Alhambra General Plan'): [101651],\n",
       " ('vegetable', 'Alhambra General Plan'): [98417, 101658, 210134],\n",
       " ('animal', 'Alhambra General Plan'): [124235],\n",
       " ('food', 'City of Commerce 2020 General Plan'): [8724, 98911, 575597, 621711],\n",
       " ('agriculture', 'City of Commerce 2020 General Plan'): [120021],\n",
       " ('garden', 'City of Commerce 2020 General Plan'): [],\n",
       " ('farm', 'City of Commerce 2020 General Plan'): [193528,\n",
       "  318124,\n",
       "  318336,\n",
       "  318611],\n",
       " ('fruit', 'City of Commerce 2020 General Plan'): [],\n",
       " ('vegetable', 'City of Commerce 2020 General Plan'): [],\n",
       " ('animal', 'City of Commerce 2020 General Plan'): [416624, 437928, 489387],\n",
       " ('food', 'Vernon_General_Plan'): [5954, 7710, 218443, 291871],\n",
       " ('agriculture', 'Vernon_General_Plan'): [230635],\n",
       " ('garden', 'Vernon_General_Plan'): [],\n",
       " ('farm', 'Vernon_General_Plan'): [4587,\n",
       "  4800,\n",
       "  5042,\n",
       "  104007,\n",
       "  110193,\n",
       "  110261,\n",
       "  113939,\n",
       "  292518],\n",
       " ('fruit', 'Vernon_General_Plan'): [],\n",
       " ('vegetable', 'Vernon_General_Plan'): [277829],\n",
       " ('animal', 'Vernon_General_Plan'): [34241,\n",
       "  146755,\n",
       "  147041,\n",
       "  164757,\n",
       "  290685,\n",
       "  290784,\n",
       "  290872,\n",
       "  291852]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a keyword subset dictionary for topic modeling purposes\n",
    "\n",
    "# create empty dictionary to store keyword positions lists\n",
    "textdictsub = {}\n",
    "# create empty dictionary to store created word segments around each keyword\n",
    "\n",
    "keyloc = ['food', 'garden', 'farm', 'agriculture', 'fruit', 'vegetable', 'animal']\n",
    "\n",
    "for plan in genplan:\n",
    "    for key in keyloc:\n",
    "        # identify position for each keyword in the raw text text\n",
    "        textpositionsub = [i for i in range(len(plan)) if plan.startswith(key, i)] # adapted: https://www.geeksforgeeks.org/python-all-occurrences-of-substring-in-string/#\n",
    "        # save to a dictionary\n",
    "        textdictsub[key, plan.split(\", \")[0]] = textpositionsub\n",
    "\n",
    "    # for each unique keyword mention in each plan, create a segment of +/- 200 words around the keyword\n",
    "    for keyword, positions in textdictsub.items():\n",
    "        for position in positions:\n",
    "            # add row to dict for ea unique key mention and corresponding word segments\n",
    "            allsegs[keyword, position] = plan[position-800:position+800]\n",
    "            \n",
    "        # turn dict into df: store ea segment for ea keyword in keyloc (for ea plan in genplan)\n",
    "        allsegsdf = pd.DataFrame.from_dict(allsegs, orient='index', columns=['segment'])\n",
    "\n",
    "# inspect/show    \n",
    "allsegsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b84fe12-ab2d-4be2-a6db-85b2072c80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the list of segments in the dataframe (the segment series) into a list of strings\n",
    "allsegslist = allsegsdf.seg.values\n",
    "\n",
    "# turning list of segment strings into a list of segment lists\n",
    "# list of strings: ea string is a narrow word segment surrounding each unique keyword mention\n",
    "planseglists = [[word for word in word_tokenize(seg.lower())\n",
    "                 if word not in swords and len(word)>2] for seg in allsegslist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce5c9764-4ffd-4b09-ad2e-c91fae582b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 66 total segments in planseglists.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} total segments in planseglists.'.format(len(planseglists)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018741b-9d2b-4a7f-9d7e-b04d51e41c19",
   "metadata": {},
   "source": [
    "Taking the list of segments for all unique keyword mentions for each keyword for each plan and passing it through GENSIM for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c1a34-60d2-4d55-83b6-6054f0c8b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating topic model: start w/ 5 topics\n",
    "dictionary = gensim.corpora.Dictionary(planseglists)\n",
    "corpus = [dictionary.doc2bow(planseg) for planseg in planseglists]\n",
    "# LdaMulticore uses multiple cores (thus, it runs faster); if you have problems, try replacing LdaMulticore with LdaModel\n",
    "model = gensim.models.LdaMulticore(corpus, id2word=dictionary, num_topics=5)\n",
    "\n",
    "# show topics\n",
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e623b58-acd0-4ad0-b7b8-aa25446d0fd9",
   "metadata": {},
   "source": [
    "### (D) Future Analysis: Environmental Justice at the Municipal Level: Are Food Systems Part of the Picture?\n",
    "\n",
    "This last exercise is focused on replicating the previous exercises exclusively on the Environmental Justice Elements of General Plans (for those plans that contain them). In 2016, the state of California passed legislation requiring all municipalities to include environmental justice planning in their overall city planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c0923-6c2a-43c4-9359-92ed47eb1886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
